{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from joblib import Memory\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import save_npz,load_npz\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up memory caching\n",
    "memory = Memory('./cachedir', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given:\n",
    "### [Real or Fake] : Fake Job Description Prediction\n",
    "### This dataset contains 18K job descriptions out of which about 800 are fake. The data consists of both textual information and meta-information about the jobs. The dataset can be used to create classification models which can learn the job descriptions which are fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "\n",
    "Process the dataset to prepare features for a classification model that predicts whether job descriptions are fraudulent or real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def load_data():\n",
    "    return pd.read_csv(\"../data/raw/fake_job_postings.csv\", index_col=0)\n",
    "\n",
    "# Load the DataFrame (will use cache if available)\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values\n",
    "\n",
    "- drop high missing value columns\n",
    "- drop rows depending on the importance of the columns and how much data is missing.\n",
    "- fill missning values with placeholder or mode\n",
    "\n",
    "### Drop rows/cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values\n",
    "\n",
    "Impute missing categorical and text columns with placeholders or mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Variables:\n",
    "\n",
    "Apply appropriate encoding techniques:\n",
    "- **Binary columns**: Use direct mapping (0/1).\n",
    "- **Ordinal columns**: Use label encoding.\n",
    "- **Nominal columns**: Use One-Hot Encoding, target encoding or frequency encoding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define lists for different types of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Ordinal columns using Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Nominal columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features\n",
    "\n",
    "### Feature Importance: \n",
    "\n",
    "Utilize algorithms that provide feature importance scores (like Random Forest or Gradient Boosting) to identify which features contribute most to the prediction.\n",
    "\n",
    "```python\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display features and their importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Cleaning\n",
    "   - Remove special characters and unnecessary whitespace from text columns.\n",
    "   - Convert text to lowercase for uniformity.\n",
    "   - Remove leading/trainling whitespaces\n",
    "   - Optionally, perform lemmatization or stemming on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text data into TF-IDF features\n",
    "\n",
    "There are 2 approaches:\n",
    "\n",
    "- Separate TF-IDF for Each Feature:\n",
    "\n",
    "    - Use a separate TfidfVectorizer for each text feature.\n",
    "    - Fit and transform each feature individually and then concatenate the resulting TF-IDF matrices.\n",
    "\n",
    "    ```python\n",
    "    vectorizers = {}\n",
    "    tfidf_matrices = []\n",
    "\n",
    "    for feature in text_features:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(df[feature])\n",
    "        tfidf_matrices.append(tfidf_matrix)\n",
    "        vectorizers[feature] = vectorizer\n",
    "\n",
    "    X = hstack(tfidf_matrices)\n",
    "    ```\n",
    "\n",
    "- Combined TF-IDF:\n",
    "\n",
    "    - Concatenate all text features into a single column and apply TfidfVectorizer once.\n",
    "    - This approach captures interactions between different text features.\n",
    "\n",
    "    ```python\n",
    "    combined_text = df['feature1'] + ' ' + df['feature2'] + ' ' + df['feature3']\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(combined_text)\n",
    "    ```\n",
    "\n",
    "- Pipeline with ColumnTransformer:\n",
    "\n",
    "    - Use ColumnTransformer from sklearn.compose to apply TfidfVectorizer to specific text features while leaving other features untouched.\n",
    "    - This is useful when you have a mixed dataset with text and non-text features.\n",
    "\n",
    "    ```python\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    text_features = ['feature1', 'feature2']\n",
    "    non_text_features = ['feature3', 'feature4']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', TfidfVectorizer(), text_features),\n",
    "            ('non_text', 'passthrough', non_text_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    X = pipeline.fit_transform(df)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with imbalanced dataset\n",
    "\n",
    "### Resampling Techniques:\n",
    "\n",
    "**Oversampling**:\n",
    "Increase the number of instances in the minority class, typically by duplicating existing instances or generating synthetic samples (e.g., using SMOTE).\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "**Undersampling**:\n",
    "Reduce the number of instances in the majority class to balance the dataset, which may involve randomly removing samples.\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "**Combination of Over- and Undersampling**:\n",
    "Use both techniques together to achieve a balance without excessive loss of information.\n",
    "\n",
    "```python\n",
    "rom imblearn.combine import SMOTEENN\n",
    "\n",
    "# Assuming X and y are your features and target\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sparse matrix\n",
    "save_npz('../data/processed/X_resampled.npz', X_resampled)\n",
    "\n",
    "# Save y_resampled as a DataFrame (you can also use Series)\n",
    "pd.Series(y_resampled).to_csv('../data/processedy_resampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sparse matrix\n",
    "X_resampled_loaded = load_npz('../data/processed/X_resampled.npz')\n",
    "\n",
    "# Load the target variable\n",
    "y_resampled_loaded = pd.read_csv('../data/processedy_resampled.csv').values.flatten()  # Use .values.flatten() if you need it as a 1D array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
